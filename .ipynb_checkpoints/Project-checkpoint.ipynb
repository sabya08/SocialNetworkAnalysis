{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PROJECT : Analysis of Users Preferences between Android and iOS \n",
    "Here we'll implement a sentiment analysis analysisng preferences of users between Android and iOS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json, time, sys, io, pickle\n",
    "import re\n",
    "import tweepy\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import hashlib\n",
    "import glob\n",
    "import math\n",
    "from decimal import *\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "access_token = \"97387398-09aWi5l77s95Vy7Jag58WvQ1soy9TzoGCA0vruWH5\"\n",
    "access_token_secret = \"ZzBHbF3LjXg7Wl40VIa6nJomv1TdwQW9Am1oQRzIW0RLN\"\n",
    "consumer_key = \"Ii9orQvVcYDTzbk5feDFIuz1n\"\n",
    "consumer_secret = \"MXTqped02xYhyuer6IpO2Rt2377MRGsTHhJQnMZ9aOagGrTLnD\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TweetsListener(tweepy.StreamListener):\n",
    "\n",
    "    def __init__(self, filename=None, api=None):\n",
    "        super(TweetsListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "        self.filename = filename\n",
    "        self.stopHashTagList = [\"#androidgames\",\"#gamesinsight\"]\n",
    "        \n",
    "    def on_status(self, status): \n",
    "        text = status.text.encode('utf-8').strip().lower()\n",
    "        if not any(x in text for x in self.stopHashTagList) and not text.startswith('rt') and not text.startswith('RT'):\n",
    "            record = {'Text': text, 'Created At': str(status.created_at)}\n",
    "            if self.num_tweets < 1000:\n",
    "                try:\n",
    "                    with open(self.filename, 'a') as f:\n",
    "                        print text\n",
    "                        pickle.dump(record,f)\n",
    "                        self.num_tweets += 1\n",
    "                    return True\n",
    "                except BaseException as e:\n",
    "                    print(\"Error on_status: %s\" % str(e))\n",
    "            else:\n",
    "                return False\n",
    "        \n",
    "    def on_error(self, status):\n",
    "        print 'Error on status', status\n",
    "\n",
    "    def on_limit(self, status):\n",
    "        print 'Limit threshold exceeded', status\n",
    "\n",
    "    def on_timeout(self, status):\n",
    "        print 'Stream disconnected; continuing...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stream = Stream(auth, TweetsListener(filename = 'data_android.json'))\n",
    "# stream.filter(track=['android','androidvsios','iosvsandroid'],languages=['en'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stream = Stream(auth, TweetsListener(filename = 'data_both.json'))\n",
    "# stream.filter(track=['AndroidVSiOS','googlevsios','iosvsgoogle','applevsandroid','iphoneisbetter','androidisbetter'],languages=['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stream = Stream(auth, TweetsListener(filename = 'data_ios.json'))\n",
    "# stream.filter(track=['iphone','ios','androidvsios','iosvsandroid'],languages=['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readTweets(filename):\n",
    "    list = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for x in range(0,1000):\n",
    "            list.append(pickle.load(f)['Text'])\n",
    "    return list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_with_not(text):\n",
    "    \"\"\"Does the same thing as tokenize_with_punct, with the following difference:\n",
    "    whenever the term 'not' appears, change the two subsequent tokens to have the prefix\n",
    "    'not_' prior to the token. See the example below. You may call \n",
    "    tokenize_with_punct as a subroutine.\n",
    "    Params:\n",
    "        text....a string\n",
    "    Returns:\n",
    "        a list of tokens\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    text = re.sub(r'[\\x85]',\"\",text)\n",
    "    tokens = tokenize_with_punct(text)\n",
    "    i=0\n",
    "    while i< len(tokens):\n",
    "        if tokens[i] == \"not\":\n",
    "            if(i+1 < len(tokens)):\n",
    "                tokens[i+1] = 'not_' + tokens[i+1]\n",
    "            if(i+2 < len(tokens)):\n",
    "                tokens[i+2] = 'not_' + tokens[i+2]\n",
    "            i = i+3\n",
    "        else:\n",
    "            i = i+1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeListToTextFile(list,filename):\n",
    "    f = open(filename, \"w\")\n",
    "    for item in list:\n",
    "      print>>f, item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list_android = readTweets('data_android.json')\n",
    "# writeListToTextFile(list_android,'data_android.txt')\n",
    "# list_ios = readTweets('data_ios.json')\n",
    "# writeListToTextFile(list_ios,'data_ios.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processTweet(tweet):\n",
    "    # process the tweets\n",
    "\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "#end\n",
    "\n",
    "#Read the tweets one by one and process it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preProcessTweets(filename):\n",
    "    fp = open(filename, 'r')\n",
    "    fpn = open('process'+filename,'w')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        processedTweet = processTweet(line)\n",
    "        print>>fpn, processedTweet\n",
    "        line = fp.readline()\n",
    "    #end loop\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preProcessTweets('data_android.txt')\n",
    "# preProcessTweets('data_ios.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replaceTwoOrMore(s):\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStopWordList(stopWordListFileName):\n",
    "    stopWords = []\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "\n",
    "    fp = open(stopWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    #print line\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatureVector(tweet,stopWords):\n",
    "    featureVector = []\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        w = replaceTwoOrMore(w)\n",
    "        w = w.strip('\\'\"?,.')\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "        if(w in stopWords or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopWords = getStopWordList('stopwords.txt')\n",
    "def featureExtraction(filename):\n",
    "    featureList = []\n",
    "    tweets = []\n",
    "    fp = open(filename,'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        linearray = line.split('-',1)\n",
    "        sentiment = linearray[0]\n",
    "        tweet = linearray[1]\n",
    "        processedTweet = processTweet(tweet)\n",
    "        featureVector = getFeatureVector(processedTweet,stopWords)\n",
    "        featureList.extend(featureVector)\n",
    "        tweets.append((featureVector,sentiment))\n",
    "        line = fp.readline()  \n",
    "    return(tweets,featureList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStatisticsAboutTweets(filename):\n",
    "    fp = open(filename,'r')\n",
    "    tweets = []\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        tweets.append(line)\n",
    "        line = fp.readline()\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predictUserPreferences(trainFileName, testFileName):\n",
    "    featureList = []\n",
    "    tweets,featureList = featureExtraction(trainFileName)\n",
    "    featureList = list(set(featureList))\n",
    "    training_set =  nltk.classify.util.apply_features(extract_features, tweets)\n",
    "    NBClassifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "    analyzedTweets = getStatisticsAboutTweets(testFileName)\n",
    "    print \"Total tweets analyzed : \" + str(len(analyzedTweets))\n",
    "    predictedNegativeTweetsLabel = []\n",
    "    predictedPositiveTweetsLabel = []\n",
    "    predictedNeutralTweetsLabel = []\n",
    "    for tweet in analyzedTweets:\n",
    "        processedTestTweet = processTweet(tweet)\n",
    "        label = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet,stopWords)))\n",
    "        if label == 'n':\n",
    "            predictedNegativeTweetsLabel.append((tweet,label))\n",
    "        elif label == 'p':\n",
    "            predictedPositiveTweetsLabel.append((tweet,label))\n",
    "        elif label == 'ne':\n",
    "            predictedNeutralTweetsLabel.append((tweet,label))\n",
    "            \n",
    "    percentPositive = (len(predictedPositiveTweetsLabel)/len(analyzedTweets)) * 100\n",
    "    percentNegative = (len(predictedNegativeTweetsLabel)/len(analyzedTweets)) * 100\n",
    "    percentNeutral = (len(predictedNeutralTweetsLabel)/len(analyzedTweets)) * 100\n",
    "    \n",
    "    print \"Positive Percentage \" + str(percentPositive)\n",
    "    print \"Negative Percentage \" + str(percentNegative)\n",
    "    print \"Neutral Percentage \" + str(percentNeutral)\n",
    "    \n",
    "    if(percentPositive > percentNegative):\n",
    "        print \"Users prefer iOS more!!\"\n",
    "    elif(percentPositive < percentNegative):\n",
    "        print \"Users dont prefer iOS!!\"\n",
    "    elif(percentPositive == percentNegative):\n",
    "        print \"User preferences are neutral\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets analyzed : 1332\n",
      "Positive Percentage 6.15615615616\n",
      "Negative Percentage 1.35135135135\n",
      "Neutral Percentage 92.4924924925\n",
      "Users prefer iOS more!!\n"
     ]
    }
   ],
   "source": [
    "predictUserPreferences('data/newdata_ios.txt','data_ios.txt')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets analyzed : 1306\n",
      "Positive Percentage 6.58499234303\n",
      "Negative Percentage 2.60336906585\n",
      "Neutral Percentage 90.8116385911\n",
      "Users prefer iOS more!!\n"
     ]
    }
   ],
   "source": [
    "predictUserPreferences('data/newdata_android.txt','data_android.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Verifying Accuracy with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 3 and last 3 labels are: [-1 -1 -1  1  0  1]\n"
     ]
    }
   ],
   "source": [
    "def get_true_labels(file_name):\n",
    "    \"\"\"Return a *numpy array* of ints for the true sentiment labels of each file.\n",
    "    1 means positive, 0 means negative. Use the name of the file to determine\n",
    "    the true label.\n",
    "    Params:\n",
    "        file_name....file containing labelled tweets\n",
    "    Returns:\n",
    "        a numpy array of 1 or 0 values corresponding to each element\n",
    "        of file_names, where 1 indicates a positive review, and 0\n",
    "        indicates a negative review.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    array = []\n",
    "    fp = open(file_name,'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        linearray = line.split('-')\n",
    "        if linearray[0] == 'p':\n",
    "            array.append(1)\n",
    "        elif linearray[0] == 'n':\n",
    "            array.append(0)\n",
    "        elif linearray[0] == 'ne':\n",
    "            array.append(-1)\n",
    "        line = fp.readline()\n",
    "    values = np.array(array)\n",
    "    return values\n",
    "    ###\n",
    "\n",
    "labels = get_true_labels('data/newdata_ios.txt')\n",
    "print('first 3 and last 3 labels are: %s' % str(labels[[1,2,3,-3,-2,-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'how',\n",
       " 's',\n",
       " 'it',\n",
       " 'going',\n",
       " 'an_underscore',\n",
       " 'is',\n",
       " 'not',\n",
       " 'really',\n",
       " 'punctuation']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Given a string, return a list of tokens such that: (1) all\n",
    "    tokens are lowercase, (2) all punctuation is removed. Note that\n",
    "    underscore (_) is not considered punctuation.\n",
    "    UPDATE: To be more specific, a token is a sequence of \n",
    "    alphanumeric characters, i.e., [A-Za-z0-9_]. Non-ascii characters\n",
    "    are not considered to be part of tokens.\n",
    "    Params:\n",
    "        text....a string\n",
    "    Returns:\n",
    "        a list of tokens\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "#     text = re.sub(r'[\\x85]',\"\",text)\n",
    "    return re.sub('\\W+', ' ', text.lower()).split()\n",
    "    ###\n",
    "\n",
    "tokenize(\"Hi! How's it going??? an_underscore is not *really* punctuation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix represents 522 documents with 1810 features\n",
      "first doc has terms:\n",
      "[90, 127, 133, 143, 145, 389, 403, 405, 438, 452, 649, 760, 815, 827, 863, 1110, 1192, 1407, 1440, 1448, 1504, 1668, 1685, 1691]\n"
     ]
    }
   ],
   "source": [
    "def do_vectorize(filename, tokenizer_fn=tokenize, min_df=1,\n",
    "                 max_df=1., binary=True, ngram_range=(1,1)):\n",
    "    \"\"\"\n",
    "    Convert a list of filenames into a sparse csr_matrix, where\n",
    "    each row is a file and each column represents a unique word.\n",
    "    Use sklearn's CountVectorizer: http://goo.gl/eJ2PJ5\n",
    "    Params:\n",
    "        filenames.......list of review file names\n",
    "        tokenizer_fn....the function used to tokenize each document\n",
    "        min_df..........remove terms from the vocabulary that don't appear\n",
    "                        in at least this many documents\n",
    "        max_df..........remove terms from the vocabulary that appear in more\n",
    "                        than this fraction of documents\n",
    "        binary..........If true, each documents is represented by a binary\n",
    "                        vector, where 1 means a term occurs at least once in \n",
    "                        the document. If false, the term frequency is used instead.\n",
    "        ngram_range.....A tuple (n,m) means to use phrases of length n to m inclusive.\n",
    "                        E.g., (1,2) means consider unigrams and bigrams.\n",
    "    Return:\n",
    "        A tuple (X, vec), where X is the csr_matrix of feature vectors,\n",
    "        and vec is the CountVectorizer object.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    fp = open(filename,'r')\n",
    "    contentList = []\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        contentList.append(line)\n",
    "        line = fp.readline()\n",
    "    vectorizer = CountVectorizer(input = 'content', tokenizer = tokenizer_fn, max_df = max_df, min_df = min_df, binary = binary, dtype = int)\n",
    "    X = vectorizer.fit_transform(contentList)\n",
    "    return (X,vectorizer)\n",
    "    ###\n",
    "    \n",
    "matrix, vec = do_vectorize('data/newdata_ios.txt')\n",
    "print ('matrix represents %d documents with %d features' % (matrix.shape[0], matrix.shape[1]))\n",
    "print('first doc has terms:\\n%s' % (str(sorted(matrix[0].nonzero()[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first shuffled document n-Hmm, my iPhone isn't doing stuff as \"smooth\" as before\n",
      " has label 0 and terms: [284, 323, 539, 795, 863, 877, 1098, 1101, 1447, 1492, 1504]\n"
     ]
    }
   ],
   "source": [
    "def repeatable_random(seed):\n",
    "    hash = str(seed)\n",
    "    while True:\n",
    "        hash = hashlib.md5(hash).digest()\n",
    "        for c in hash:\n",
    "            yield ord(c)\n",
    "\n",
    "def repeatable_shuffle(X, y, filenames):\n",
    "    r = repeatable_random(42) \n",
    "    indices = sorted(range(X.shape[0]), key=lambda x: next(r))\n",
    "    return X[indices], y[indices], np.array(filenames)[indices]\n",
    "\n",
    "trainTweets = []\n",
    "trainiOSFiles = open('data/newdata_ios.txt','r')\n",
    "trainiOSTweet = trainiOSFiles.readline()\n",
    "while trainiOSTweet:\n",
    "    trainTweets.append(trainiOSTweet)\n",
    "    trainiOSTweet = trainiOSFiles.readline()\n",
    "\n",
    "    \n",
    "X, y, filenames = repeatable_shuffle(matrix, labels, trainTweets)\n",
    "\n",
    "print('first shuffled document %s has label %d and terms: %s' % \n",
    "      (filenames[0], y[0], sorted(X[0].nonzero()[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clf():\n",
    "    return LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 accuracy = 1.0\n",
      "Fold 1 accuracy = 1.0\n",
      "Fold 2 accuracy = 1.0\n",
      "average cross validation accuracy=1.0000\n"
     ]
    }
   ],
   "source": [
    "def do_cross_validation(X, y, n_folds=3, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform n-fold cross validation, calling get_clf() to train n\n",
    "    different classifiers. Use sklearn's KFold class: http://goo.gl/wmyFhi\n",
    "    Be sure not to shuffle the data, otherwise your output will differ.\n",
    "    Params:\n",
    "        X.........a csr_matrix of feature vectors\n",
    "        y.........the true labels of each document\n",
    "        n_folds...the number of folds of cross-validation to do\n",
    "        verbose...If true, report the testing accuracy for each fold.\n",
    "    Return:\n",
    "        the average testing accuracy across all folds.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    cv = KFold(len(y), n_folds)\n",
    "    i = 0\n",
    "    accuracies = []\n",
    "    for train_idx, test_idx in cv:\n",
    "        clf = get_clf()\n",
    "        clf.fit(X[train_idx], y[train_idx])\n",
    "        predicted = clf.predict(X[test_idx])\n",
    "        acc = accuracy_score(y[test_idx], predicted)\n",
    "        if verbose :\n",
    "            print(\"Fold {} accuracy = {}\".format(i,acc))\n",
    "        i = i + 1\n",
    "        accuracies.append(acc)\n",
    "    avg = np.mean(accuracies)\n",
    "    return avg\n",
    "    ###\n",
    "    \n",
    "print('average cross validation accuracy=%.4f' %\n",
    "      do_cross_validation(X, y, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_expt(filenames, y, tokenizer_fn=tokenize,\n",
    "            min_df=1, max_df=1., binary=True,\n",
    "            ngram_range=(1,1), n_folds=5):\n",
    "    \"\"\"\n",
    "    Run one experiment, which consists of vectorizing each file,\n",
    "    performing cross-validation, and returning the average accuracy.\n",
    "    You should call do_vectorize and do_cross_validation here.\n",
    "    Params:\n",
    "        filenames.......list of review file names\n",
    "        y...............the true sentiment labels for each file\n",
    "        tokenizer_fn....the function used to tokenize each document\n",
    "        min_df..........remove terms from the vocabulary that don't appear\n",
    "                        in at least this many documents\n",
    "        max_df..........remove terms from the vocabulary that appear in more\n",
    "                        than this fraction of documents\n",
    "        binary..........If true, each documents is represented by a binary\n",
    "                        vector, where 1 means a term occurs at least once in \n",
    "                        the document. If false, the term frequency is used instead.\n",
    "        ngram_range.....A tuple (n,m) means to use phrases of length n to m inclusive.\n",
    "                        E.g., (1,2) means consider unigrams and bigrams.\n",
    "        n_folds.........The number of cross-validation folds to use.\n",
    "    Returns:\n",
    "        the average cross validation testing accuracy.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    matrix,vec=do_vectorize(filenames,tokenizer_fn, min_df, max_df, binary, ngram_range)\n",
    "    accuracy=do_cross_validation(matrix, y, n_folds, verbose=False)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy using default settings: 0.7739\n"
     ]
    }
   ],
   "source": [
    "print('accuracy using default settings: %.4g' % do_expt('data/newdata_ios.txt', y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index (521) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-96211efa56d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy using default settings: %.4g'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdo_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/newdata_android.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-108-0b8debde59ab>\u001b[0m in \u001b[0;36mdo_expt\u001b[0;34m(filenames, y, tokenizer_fn, min_df, max_df, binary, ngram_range, n_folds)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m###TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0maccuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-d3d883ffb59e>\u001b[0m in \u001b[0;36mdo_cross_validation\u001b[0;34m(X, y, n_folds, verbose)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sabyasac/anaconda/lib/python2.7/site-packages/scipy/sparse/csr.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;31m# [[1,2],??]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                 \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# [[1,2],j] or [[1,2],1:2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sabyasac/anaconda/lib/python2.7/site-packages/scipy/sparse/csr.pyc\u001b[0m in \u001b[0;36mextractor\u001b[0;34m(indices, N)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mmin_indx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_indx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmin_indx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sabyasac/anaconda/lib/python2.7/site-packages/scipy/sparse/csr.pyc\u001b[0m in \u001b[0;36mcheck_bounds\u001b[0;34m(indices, N)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mmax_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_indx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index (%d) out of range'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax_indx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mmin_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index (521) out of range"
     ]
    }
   ],
   "source": [
    "print('accuracy using default settings: %.4g' % do_expt('data/newdata_android.txt', y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
